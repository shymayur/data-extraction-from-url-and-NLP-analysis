{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7c90a1",
   "metadata": {},
   "source": [
    "# Final Project BlackCoffer: Data Extraction and NLP\n",
    "## (The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5998eb3",
   "metadata": {},
   "source": [
    "## Candidate Name: Mayur Kumar Sharma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf569f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "414003b7",
   "metadata": {},
   "source": [
    "## 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024c04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import cmudict\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525829de",
   "metadata": {},
   "source": [
    "## 2. Reading the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a2fef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workbook = pd.read_excel('input1.xlsx')\n",
    "workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0873e42",
   "metadata": {},
   "source": [
    "### Note: due to bad weather at my location, the internet speed was slow, hence created small input file as \"input1\", & performed the necessary actions on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89dadfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the elements\n",
    "url_id = []\n",
    "url = []\n",
    "\n",
    "for i in range(2):\n",
    "    item1 = workbook['URL_ID'][i]\n",
    "    item2 = workbook['URL'][i]\n",
    "    url_id.append(item1)\n",
    "    url.append(item2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be54b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 38]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2608ac26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/',\n",
       " 'https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1a83f",
   "metadata": {},
   "source": [
    "## 3. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd060db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists('BlackOutput'):\n",
    "    os.makedirs('BlackOutput')\n",
    "    \n",
    "pps = []\n",
    "nps = []\n",
    "pol = []\n",
    "sub = []\n",
    "asl = []\n",
    "pcw = []\n",
    "fog = []\n",
    "words_per_sent = []\n",
    "words_d = []\n",
    "spw = []\n",
    "pro = []\n",
    "word_len = []\n",
    "\n",
    "# Iterate over the links\n",
    "for i, link in enumerate(url):\n",
    "\n",
    "    # Fetch the HTML content of the page\n",
    "    response = requests.get(link)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the text from the HTML using the get_text() method\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Save the extracted text to a file\n",
    "    with open(f'BlackOutput/{url_id[i]}.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Opening the file as raw_doc\n",
    "    f = open(f'BlackOutput/{url_id[i]}.txt', 'r', errors='ignore')\n",
    "    raw_doc = f.read()\n",
    "    raw_doc = raw_doc.lower()\n",
    "    \n",
    "    \n",
    "    # Performing the tokenization \n",
    "    sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "    word_tokens = nltk.word_tokenize(raw_doc)\n",
    "    \n",
    "    \n",
    "    # Updating the STOP-WORD list\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    for each in [',', '.', '{', '}', '[', ']', '(', ')', '/', '\\n', '-','''\"''', '&','for', '?', '!', '|', '&']:\n",
    "        stop_words.add(each)\n",
    "    \n",
    "    \n",
    "    # Removal of stop_words \n",
    "    list_of_words = word_tokenize(raw_doc)\n",
    "    filtered_list = []   #We will collect the filtered words here \n",
    "    for word in list_of_words:\n",
    "        if word.casefold() not in stop_words:\n",
    "            filtered_list.append(word.lower())\n",
    "            \n",
    "    text_for_polarity = ' '.join(filtered_list)\n",
    "    \n",
    "    \n",
    "    # Defining instance of Sentiment Analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    polarity = sia.polarity_scores(text_for_polarity)\n",
    "    \n",
    "################################################################################################################################\n",
    "\n",
    "    # Calculating the required variables\n",
    "    \n",
    "################################################################################################################################    \n",
    "\n",
    "    #1. \n",
    "    positive_polarity_score = polarity['pos']\n",
    "    \n",
    "    #2.\n",
    "    negative_polarity_score = polarity['neg']\n",
    "    \n",
    "    #3 & #4.\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    # Calculate the polarity score\n",
    "    polarity_score = blob.sentiment.polarity   \n",
    "    # Calculate the subjectivity score\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    \n",
    "    #5 & #6. Average Sentance Length & Total Words Count ###########################################\n",
    "    # Split the content into sentences using a sentence tokenizer\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Calculate the total number of sentences\n",
    "    total_sentences = len(sentences)    \n",
    "    # Calculate the total number of words in the text\n",
    "    words = word_tokenize(text)\n",
    "    total_words = len(words)    \n",
    "    # Calculate the average sentence length\n",
    "    avg_sentence_length = total_words / total_sentences\n",
    "    \n",
    "    #7. Percent Complex Words #################################################################\n",
    "    # Tokenize the text into words\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # Load the CMU Pronouncing Dictionary for counting syllables\n",
    "    cmud = cmudict.dict()    \n",
    "    # Count the number of complex words (words with three or more syllables)\n",
    "    num_complex_words = sum([1 for word in words if len(cmud.get(word.lower(), [])) >= 3])\n",
    "    # Calculate the total number of words\n",
    "    # num_words = len(words)\n",
    "    # Calculate the percentage of complex words\n",
    "    percentage_complex_words = (num_complex_words / total_words) * 100\n",
    "    \n",
    "    #8. Fog Index #############################################################################\n",
    "    fog_index = textstat.gunning_fog(text)\n",
    "    \n",
    "    \n",
    "    #9. Syllables Per Word ####################################################################\n",
    "    #split by whitespace\n",
    "    words_split = text.split()  \n",
    "    \n",
    "    # load cmudict\n",
    "    d = cmudict.dict()\n",
    "    # function to count syllables in a word\n",
    "    def count_syllables(word):\n",
    "        # remove non-alphabetic characters from word\n",
    "        word = ''.join(filter(str.isalpha, word.lower()))\n",
    "        # check if word is in cmudict\n",
    "        if word in d:\n",
    "            # count the number of syllables in word\n",
    "            return len(list(filter(lambda s: s[-1].isdigit(), d[word][0])))\n",
    "        else:\n",
    "            # if word is not in cmudict, return 1\n",
    "            return 1\n",
    "    # tokenize text into words\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # count syllables and words\n",
    "    syllables = sum([count_syllables(w) for w in words])\n",
    "    # words_count = len(words)\n",
    "    # calculate syllables per word\n",
    "    syllables_per_word = syllables / total_words\n",
    "    \n",
    "    #10. Pronouns ############################################################################\n",
    "    # define personal pronouns\n",
    "    personal_pronouns = ['I', 'me', 'mine', 'myself', 'we', 'us', 'ours', 'ourselves']\n",
    "    # count the number of personal pronouns\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word.lower() in personal_pronouns:\n",
    "            count += 1\n",
    "    counts = count\n",
    "    \n",
    "    #11. Average Word Length\n",
    "    # words = text.split()\n",
    "    total_chars = sum(len(word) for word in words_split)\n",
    "    avg_word_length = total_chars / len(words)\n",
    "\n",
    "    ##########################################################################################################################\n",
    "    \n",
    "    # Appending into list variables\n",
    "    \n",
    "    ##########################################################################################################################\n",
    "    \n",
    "    pps.append(positive_polarity_score)\n",
    "    nps.append(negative_polarity_score)\n",
    "    pol.append(polarity_score)\n",
    "    sub.append(subjectivity_score)\n",
    "    \n",
    "    asl.append(avg_sentence_length)\n",
    "    pcw.append(percentage_complex_words)\n",
    "    fog.append(fog_index)\n",
    "    words_per_sent.append(avg_sentence_length)\n",
    "    \n",
    "    words_d.append(total_words)\n",
    "    spw.append(syllables_per_word)\n",
    "    pro.append(counts)\n",
    "    word_len.append(avg_word_length)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0bd3f",
   "metadata": {},
   "source": [
    "## 4. Creating Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e3d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percent of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Words per Sentence</th>\n",
       "      <th>Words Count</th>\n",
       "      <th>Syllable Per Word</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Average Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.145101</td>\n",
       "      <td>0.440490</td>\n",
       "      <td>36.070588</td>\n",
       "      <td>8.447489</td>\n",
       "      <td>16.90</td>\n",
       "      <td>36.070588</td>\n",
       "      <td>3066</td>\n",
       "      <td>1.704501</td>\n",
       "      <td>27</td>\n",
       "      <td>5.305610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.409129</td>\n",
       "      <td>28.414894</td>\n",
       "      <td>10.146013</td>\n",
       "      <td>13.52</td>\n",
       "      <td>28.414894</td>\n",
       "      <td>2671</td>\n",
       "      <td>1.558592</td>\n",
       "      <td>31</td>\n",
       "      <td>4.788469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  Positive Score  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...           0.176   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...           0.198   \n",
       "\n",
       "   Negative Score  Polarity Score  Subjectivity Score  \\\n",
       "0           0.038        0.145101            0.440490   \n",
       "1           0.058        0.108266            0.409129   \n",
       "\n",
       "   Average Sentence Length  Percent of Complex Words  Fog Index  \\\n",
       "0                36.070588                  8.447489      16.90   \n",
       "1                28.414894                 10.146013      13.52   \n",
       "\n",
       "   Words per Sentence  Words Count  Syllable Per Word  Personal Pronouns  \\\n",
       "0           36.070588         3066           1.704501                 27   \n",
       "1           28.414894         2671           1.558592                 31   \n",
       "\n",
       "   Average Word Length  \n",
       "0             5.305610  \n",
       "1             4.788469  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding respective coulmns and feeding the respective variables for the given input file\n",
    "\n",
    "workbook['Positive Score'] = pps\n",
    "workbook['Negative Score'] = nps\n",
    "workbook['Polarity Score'] = pol\n",
    "workbook['Subjectivity Score'] = sub\n",
    "workbook['Average Sentence Length'] = asl\n",
    "workbook['Percent of Complex Words'] = pcw\n",
    "workbook['Fog Index'] = fog\n",
    "workbook['Words per Sentence'] = words_per_sent\n",
    "workbook['Words Count'] = words_d\n",
    "workbook['Syllable Per Word'] = spw\n",
    "workbook['Personal Pronouns'] = pro\n",
    "workbook['Average Word Length'] = word_len\n",
    "\n",
    "\n",
    "workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538afa4",
   "metadata": {},
   "source": [
    "## 5. Exporting the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec190700",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Mayur/Desktop/BlackCofferProject/output.xlsx'\n",
    "workbook.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9a580",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddbdc0f",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fbf2f",
   "metadata": {},
   "source": [
    "### (Mayur Kumar Sharma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
